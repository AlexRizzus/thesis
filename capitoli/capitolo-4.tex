% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../tesi.tex

%**************************************************************
\chapter{\textit{Chaos Engineering} nel progetto aziendale}
\label{cap:analisi-requisiti}
%**************************************************************

\intro{In questo capitolo tratto di come la teoria esposta nei capitoli precedenti sia stata applicata durante lo \textit{stage} e dei compromessi che sono stati fatti tra la teoria e l'utilizzo pratico.}

\section{Progettazione di MICO2}

Abbiamo deciso di analizzare il problema attraverso il Domain Driven Development per assicurarci di averlo compreso bene e al fine di ragionare sul come applicare la teoria studiata in queste prime settimane. 
Una suddivisione ad alto livello del dominio di MICO2 ha portato a definire 3 sotto-domini: \textit{client} Interaction Domain: si occupa di ricevere le richieste dal \textit{client} e di comunicare col \textit{database} Interaction Domain e con l’External System Domain per processare la richiesta e ottenere la risposta da inviare al \textit{client} che ha effettuato la richiesta; 
\textit{database} Interaction Domain: si occupa di ricevere la richiesta di lettura o scrittura di un dato complesso ( per esempio un flow) e orchestrare tutte le richieste al DB e comporre il dato finale da restituire al chiamante, ovvero il \textit{client} Interaction Domain; 
External System Domain: si occupa di gestire richieste ai servizi esterni per verificare lo stato dei vari step di un flusso che poi restituirà al chiamante, ovvero il \textit{client} Interaction Domain. L'idea finale prevede quindi un attore che riceve la richiesta dal \textit{client} e si occupa di generare un attore che prende in carico la richiesta, quest'ultimo interroga dunque il \textit{database} e costruisce l'oggetto ( ad esempio flow ) da restituire all'attore padre che risponderà al chiamante.

Ci siamo poi spinti più nel dettaglio per avere uno schema che rappresentasse tutte le entità che interagivano nel nostro servizio, nel caso specifico del \textit{framework} \gls{akkag} parliamo di attori.
\begin{center}
    \includegraphics[width=14cm]{attori_mico2.png}
    \label{tab:schema-attori-mico2}
\end{center}

Dopo aver ottenuto uno schema ad alto livello e uno più specifico del sistema abbiamo ragionato sui possibili rischi legati alla nostra architettura e li abbiamo inseriti in una tabella ottenendo il seguente risultato:
\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{chaos-risk-table.PNG}
    \caption{schema di FIT}
    \label{tab:fitschema}
\end{figure}
Il punto che sicuramente ci è apparso più critico in primo luogo era la comunicazione con i servizi esterni e con il \textit{database}, questi sistemi infatti essendo indipendenti da MICO2 potrebbero avere dei malfunzionamenti e causare un disservizio all'utente finale.
Come soluzione a questi primi due problemi abbiamo pensato alla creazione di risposte positive del sistema che notifichino la presenza di problemi ai sistemi esterni o al \textit{database} se ve ne fossero.
In secondo luogo abbiamo pensato ai problemi interni che MICO2 poteva avere, in particolare come l'applicazione si comporta in scarsità di risorse come CPU o con un aumento del traffico in entrata, in particolare temevamo che questo potesse causare l'ammanco di alcune risposte o un ritardo nella consegna delle stesse.
In particolare l'aumento di latenza nelle risposte è il rischio che più facilmente si può verificare e dunque uno in cui abbiamo posto molta attenzione durante gli esperimenti.
Per finire abbiamo pensato a che problemi si potessero verificare nel \textit{deploy} dell'applicazione anticipando un pò i tempi, sia su come vengano gestite le risorse fisiche che i \textit{pod} e i nodi di Kubernetes, aspetti che poi siamo andati a controllare con gli esperimenti.
In conclusione come team di sviluppo abbiamo ritenuto molto utile questa pratica, ci ha permesso di individuare problemi a cui non avevamo pensato e in alcuni casi anche di trovare delle soluzioni applicabili durante lo sviluppo, risparmiando così tempo prezioso.

\section{Compromessi della progettazione}
Purtroppo durante l'esperienza di \textit{stage} avevamo a che fare con un \textit{software} di dimensioni ridotte per cui la lista di problemi individuati non si è rivelata essere molto lunga.
Inoltre rispetto alla FMEA worksheet citata nel capitolo precedente quella da noi prodotta è una versione ridotta che tiene conto solamente della probabilità, tuttavia abbiamo ritenuto che questa versione fosse sufficiente per lo scopo dello \textit{stage}.
Concludendo la progettazione secondo la \textit{Chaos Engineering} porta indubbiamente dei vantaggi per il team anche se non viene adottata in maniera completa.

\section{\textit{Deploy} dell'applicazione}
Una volta terminato lo sviluppo di MICO2 ne abbiamo effettuato il \textit{deploy} in un cluster Kubernetes all'interno di AWS.
Abbiamo effettuato il \textit{deploy} di due applicazioni separate, una contentente l'applicazione MICO2 e un'altra con il \textit{database} postgreSQL di MICO2.
Il \textit{deploy} di MICO2 prevede un LoadBalancer che esponga un \textit{endpoint} per l'applicazione e distribuisca il traffico in arrivo, sono anche previste tre \gls{replicasg} dell'applicazione.
Con le \gls{replicasg} puntiamo ad avere sempre il servizio disponibile anche nel caso in cui alcuni \textit{pod} abbiano un malfunzionamento tramite il LoadBalancer che redirige il traffico tra i \textit{pod} attualmente attivi.
Per il \textit{database} invece abbiamo fatto il \textit{deploy} un servizio che espone l'\textit{endpoint} del \textit{database} e una sua sola istanza in un \textit{pod}.

\section{Test di \textit{Chaos Engineering}}
Con il \textit{deploy} pronto abbiamo iniziato ad approcciarci agli esperimenti di \textit{Chaos Engineering}, innanzitutto abbiamo preso in mano la tabella stilata durante la progettazione e abbiamo iniziato a preparare l'ambiente e gli strumenti per effettuare i test.

Tra gli strumenti descritti nel capitolo precedente abbiamo deciso di appoggiarci a ChaosToolkit per eseguire i nostri esperimenti, in particolare sfruttando le azioni comprese nel pacchetto ChaosToolkitK8s per inserire eventi nel cluster.

Per simulare il traffico di produzione ci siamo avvalsi di uno strumento opensource chiamato \gls{go-wrkg} che è capace di generare un carico molto pesante di richieste anche se eseguito su una sola CPU, lo abbiamo usato per simulare un traffico superiore a quello di produzione durante gli esperimenti; il traffico di produzione massimo previsto dai tutor infatti era di circa 20 richieste al secondo, per i nostri test invece abbiamo inviato in media 40 richieste al secondo.
Abbiamo interrogato sempre l'\textit{endpoint} più oneroso per il sistema utilizzando il comando:
\begin{verbatim}
    go-wrk -T 5000 -c 10 [mico2-address]/flows
\end{verbatim}

che ci ha permesso di simulare il traffico con 10 \gls{goroutinesg} che mandavano richieste in contemporanea per 5 secondi.

Tenendo a mente che lo scopo principale della \textit{Chaos Engineering} è aumentare la fiducia del team nel prodotto abbiamo deciso di iniziare con un esperimento semplice che prevedeva la terminazione di un \textit{pod} dell'applicazione.
Lo scopo era osservare come il tempo medio di risposta veniva influenzato da questo evento, l'esperimento ha prodotto un esito positivo in quanto il servizio è rimasto attivo e i tempi medi di risposta non sono cambiati in modo significativo.


Dopo il primo esperimento con un \textit{blast radius} molto basso abbiamo deciso di aumentarlo nei due esperimenti successivi andando ad incrementare il numero di replicas a 10 e terminando rispettivamente 9 e 10 \textit{pod}.
Nel secondo esperimento appunto abbiamo terminato in modo molto analogo al primo 9 \textit{pod} dal \textit{deploy} simulando sempre un traffico superiore a quello di produzione, anche in questo caso il sistema è riuscito comunque a gestire con successo tutte le richieste inviate anche se il tempo di risposta è aumentato in modo significativo passando da circa 200 millisecondi a più di 1 secondo.
Dopo aver discusso il risultato del secondo esperimento ci è parso chiaro che fosse necessario definire una policy per il riavvio dei \textit{pod} in caso di errori o terminazioni, dopo averla definita se il \textit{deploy} aveva un numero di \textit{pod} attivi inferiore a quello definito questi venivano creati appena possibile; Con questo nuovo strumento abbiamo provato ad effettuare nuovamente il test e abbiamo osservato come questa volta il tempo medio di risposta aumentasse solo di 300 millisecondi dandoci maggiore fiducia nelle capacità del sistema di riprendersi da una situazione indesiderata.


Con il terzo esperimento invece abbiamo voluto osare e provare a terminare in contemporanea tutti i \textit{pod} del \textit{deploy} per osservare quante delle richieste in arrivo avrebbero avuto problemi e per quanto tempo il sistema non sarebbe stato in grado di rispondere alle richieste.
Purtroppo stavolta il sistema non è rimasto attivo per tutta la durata dell'esperimento, il \textit{report} finale di go-wrk infatti ha segnalato come 14 richieste non siano andate a buon fine.
\begin{figure}[h]
    \centering
    \includegraphics[width=14cm]{all_pods_killed.png}
    \caption{Report del \textit{tool} go-wrk per il terzo esperimento}
    \label{tab:report-all-pods-killed}
\end{figure}
Dopo una breve discussione abbiamo comunque considerato il risultato dell'esperimento buono perchè un malfunzionamento contemporaneo di tutti i \textit{pod} ha portato solo alla perdita di 14 richieste su 2450 per il minuto dell'esperimento.
Con gli esperimenti successivi abbiamo deciso di cambiare focus e di spostarci sulla comunicazione col \textit{database}, volevamo essere sicuri che il sistema reagisse correttamente ad un malfunzionamento al \textit{database} o che restituisse un codice di errore appropriato.
Per farlo abbiamo simulato un malfunzionamento al \textit{database} andando a disattivare il servizio che esponeva l'endpoint del \textit{database}, non riuscendo più a contattare il \textit{database} il sistema avrebbe dovuto risponderci in maniera positiva facendoci notare l'errore o con un codice di errore specifico.
L'esperimento in questo caso è fallito, poichè la prova che avevamo definito, ossia che il codice della risposta del server fosse 200 o 503 non è stata rispettata, è stato restituito invece un errore interno del server.
\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{report-db-failure.PNG}
    \caption{report dell'esperimento sul \textit{database}}
    \label{tab:report-database-experiment}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{probe-postgres-failure.PNG}
    \caption{prova per controllare lo stato del \textit{database}}
    \label{tab:report-database-experiment}
\end{figure}

Abbiamo dunque documentato l'esito dell'esperimento e discusso sulla necessità di predisporre una risposta per gestire in maniera resiliente il malfunzionamento del \textit{database} e che per motivi di tempo abbiamo lasciato come possibile miglioramento per chi prenderà in mano il progetto.
Infine abbiamo deciso di esplorare il comportmento del sistema in scarsità di risorse, in particolare CPU e memoria RAM.

Per simulare un utilizzo intenso di CPU abbiamo utilizzato uno strumento chiamato \gls{stress-ngg} che permette di testare un sistema nei suoi sottosistemi fisici in molti modi diversi, in particolare mette a disposizione 78 possibili test per la CPU e 20 per la memoria.
Abbiamo scritto un esperimento che oltre a prevedere le prove iniziali includeva nel metodo due azioni, una che installava all'interno di ciascun \textit{pod} il \textit{software} e il secondo che eseguiva su \textit{bash} il seguente comando:
\begin{verbatim}
    stress-ng -matrix 0 -matrix-size 64  -tz -t 400 -metrics-brief
\end{verbatim}
che stressa la CPU con delle matrici di lato 64 da risolvere per 400 secondi, questo richiedeva più dell'80\% della CPU dei \textit{pod}.
Per l'esperimento della RAM invece abbiamo utilizzato il seguente comando:
\begin{verbatim}
    stress-ng --vm 1 --vm-bytes 75% --vm-method flip --verify -t 6m
\end{verbatim}

In entrambi questi esperimenti abbiamo constatato come il sistema non si sia rivelato in grado di gestire tutte le richieste in entrata in quanto la CPU e la RAM in certi momenti raggiungevano livelli critici.
\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{drain-memory-test.png}
    \caption{Report del \textit{tool} go-wrk per l'esperimento in cui simuliamo scarsità di RAM}
    \label{tab:report-drain-ram}
\end{figure}
A posteriori abbiamo quindi deciso di dichiarare un meccanismo di \textit{autoscaling} variabile da 3 a 10 \gls{replicasg} e che per scalare richiede un consumo di CPU o RAM pari al 75\% di quella massima.

\section{Compromessi durante gli esperimenti}
Il compromesso più grande che come team abbiamo dovuto fare rispetto alla teoria studiata è stato sicuramente il numero di esperimenti che abbiamo potuto fare, un pò per il tempo ristretto e un pò perchè il sistema che noi andavamo a testare non era particolarmente complesso e dunque aveva poche di quelle zone d'ombra che costituiscono il terreno d'indagine per la \textit{Chaos Engineering}.

A limitare leggermente i possibili esperimenti da effettuare sono stati anche i permessi che avevamo a disposizone, per motivi legati all'azienda non avevamo i permessi necessari per inserire eventi a livello dei nodi di Kubernetes o nel \textit{deploy} di AWS.

Nel complesso tuttavia siamo riusciti a rappresentare in una versione semplificata quello che ci aspetta da degli esperimenti di \textit{Chaos Engineering}: la preparazione, la stesura di esperimenti con step ben definiti, l'esecuzione e la discussione dei risultati insieme al resto del team.

\section{Valutazioni sul prodotto finito e spunti per il futuro}
A conclusione degli esperimenti di \textit{Chaos Engineering} e sempre nell'ottica di aumentare la fiducia nel prodotto abbiamo deciso di confrontare le \textit{performance} sotto carico della nostra versione del \textit{software} con quella precedente.
Per simulare il carico abbiamo utilizzato lo stesso strumento usato negli esperimenti di \textit{Chaos Engineering} variando il numero di \gls{goroutinesg} che inviavano richieste per osservare le perfomarce all'aumentare di queste ultime.
Dopo aver effettuato le prove abbiamo raggruppato i dati in dei grafici secondo il tempo medio di risposta all'aumentare delle richieste al secondo e all'aumentare delle goroutines che inviavano le richieste.
\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{grafico-tempo-di-risposta.png}
    \caption{Grafico rappresentante il tempo medio di rispota rispetto alle richieste al secondo}
    \label{tab:graph-req-sec}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{grafico-goroutines.png}
    \caption{Grafico rappresentante il tempo medio di rispota rispetto al numero di goroutines}
    \label{tab:graph-goroutines}
\end{figure}

Da entrambi questi grafici possiamo evincere come l'applicazione e il \textit{deploy} realizzati abbiano delle \textit{performance} superiori rispetto alla precedente versione già con poche richieste e che questo divario aumenta ulteriormente quando aumentano le richieste e le goroutines.
Come detto nel capitolo precedente abbiamo redatto un documento contenente tutti i \textit{report} ottenuti da ChaosToolkit e le riflessioni del team su ogni esperimento, molti degli esperimenti hanno evidenziato la forza e la resilienza del sistema a situazioni impreviste mentre altri hanno identificato delle criticità, alcune delle quali per motivi di tempo abbiamo lasciato documentate ma non risolte.